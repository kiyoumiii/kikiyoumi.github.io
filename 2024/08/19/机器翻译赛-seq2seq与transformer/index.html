<!-- build time:Sat Dec 28 2024 17:30:39 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Hexo" href="https://kiyoumiii.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="Hexo" href="https://kiyoumiii.github.io/atom.xml"><link rel="alternate" type="application/json" title="Hexo" href="https://kiyoumiii.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="NLP"><link rel="canonical" href="https://kiyoumiii.github.io/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/"><title>机器翻译赛-seq2seq与transformer - 机器学习 | Yume Shoka = Hexo</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">机器翻译赛-seq2seq与transformer</h1><div class="meta"><span class="item" title="创建时间：2024-08-19 16:02:58"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-08-19T16:02:58+08:00">2024-08-19</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>13k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>12 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Yume Shoka</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/12/27/BSvAynCXpt8UZIE.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/27/6xf5aH1PcoiL8gW.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/27/YtXRN3FozvqPnUD.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/27/LjZWYulnxfRm1pV.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/27/4X7VEOY9a53GzgC.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/12/27/hYJunlxKyrf5HRE.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/machine-learning/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://kiyoumiii.github.io/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="kiyoumiii"><meta itemprop="description" content=", kiyoumiii's blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Hexo"></span><div class="body md" itemprop="articleBody"><h2 id="seq2seq架构"><a class="anchor" href="#seq2seq架构">#</a> Seq2Seq 架构</h2><p>baseline 代码中实现了一个经典的序列到序列 (Seq2Seq) 模型，中间层使用的 GRU 网络，并且网络中加入了注意力机制 (Attention Mechanism)</p><p>编码器将长度可变的输入序列转换成 形状固定的上下文变量， 并且将输入序列的信息在该上下文变量中进行编码。</p><p>让我们实现循环神经网络编码器。 注意，我们使用了嵌入层（embedding layer） 来获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（vocab_size）， 其列数等于特征向量的维度（embed_size）。 对于任意输入词元的索引 i ， 嵌入层获取权重矩阵的第 i 行（从 0 开始）以返回其特征向量。 另外，本文选择了一个多层门控循环单元来实现编码器。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 如果未提及状态，则默认为0</span></span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p></p><p>下面，我们实例化上述编码器的实现： 我们使用一个两层门控循环单元编码器，其隐藏单元数为 16。 给定一小批量的输入序列 X（批量大小为 4，时间步为 7）。 在完成所有时间步后， 最后一层的隐状态的输出是一个张量（output 由编码器的循环层返回）， 其形状为（时间步数，批量大小，隐藏单元数）。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><p><code>torch.Size([7, 4, 16])</code></p><p>由于这里使用的是门控循环单元， 所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state 中还将包含记忆单元信息。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">state.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><p><code>torch.Size([2, 4, 16])</code></p><p>当实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X_and_context, state)</span><br><span class="line">        output = <span class="variable language_">self</span>.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p></p><p>下面，我们用与前面提到的编码器中相同的超参数来实例化解码器。 如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br></pre></td></tr></table></figure><p></p><p><code>(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))</code></p><p>上述循环神经网络 “编码器－解码器” 模型中的各层如:</p><p>&lt;br&gt;<br>&lt;div align=&quot;center&quot;&gt;<br>&lt;style&gt;.pjqubltdmhfu{}&lt;/style&gt;<img data-src="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/01.png" class="pjqubltdmhfu"><br>&lt;/div&gt;<br>&lt;br&gt;</p><p>在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用 softmax 来获得分布， 并通过计算交叉熵损失函数来进行优化。<br>特定的填充词元被添加到序列的末尾， 因此不同长度的序列可以以相同形状的小批量加载。 但是，我们应该将填充词元的预测排除在损失函数的计算之外。<br>为此，我们可以使用下面的 sequence_mask 函数 通过零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。 例如，如果两个序列的有效长度（不包括填充词元）分别为 1 和 2， 则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure><p></p><p><code>tensor([[1, 0, 0], [4, 5, 0]])</code></p><p>我们还可以使用此函数屏蔽最后几个轴上的所有项。如果愿意，也可以使用指定的非零值来替换这些项。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), value=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p></p><p>`<br>tensor([[[ 1., 1., 1., 1.],<br>[-1., -1., -1., -1.],<br>[-1., -1., -1., -1.]],</p><pre><code>    [[ 1.,  1.,  1.,  1.],
     [ 1.,  1.,  1.,  1.],
     [-1., -1., -1., -1.]]])
</code></pre><p>`</p><p>现在，我们可以通过扩展 softmax 交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为 1。 一旦给定了有效长度，与填充词元对应的掩码将被设置为 0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred的形状：(batch_size,num_steps,vocab_size)</span></span><br><span class="line">    <span class="comment"># label的形状：(batch_size,num_steps)</span></span><br><span class="line">    <span class="comment"># valid_len的形状：(batch_size,)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        <span class="variable language_">self</span>.reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, <span class="variable language_">self</span>).forward(</span><br><span class="line">            pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p></p><p>我们可以创建三个相同的序列来进行代码健全性检查， 然后分别指定这些序列的有效长度为 4、2 和 0。 结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = MaskedSoftmaxCELoss()</span><br><span class="line">loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long),</span><br><span class="line">     torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p></p><p><code>tensor([2.3026, 1.1513, 0.0000])</code></p><p>在下面的循环训练过程中，特定的序列开始词元（“&lt;bos&gt;”）和 原始的输出序列（不包括序列结束词元 “&lt;eos&gt;”） 拼接在一起作为解码器的输入。 这被称为强制教学（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">xavier_init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                     xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失总和，词元数量</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>],</span><br><span class="line">                          device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()      <span class="comment"># 损失函数的标量进行“反向传播”</span></span><br><span class="line">            d2l.grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">        <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><p>现在，在机器翻译数据集上，我们可以 创建和训练一个循环神经网络 “编码器－解码器” 模型用于序列到序列的学习。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p></p><p>为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（“&lt;bos&gt;”） 在初始时间步被输入到解码器中。 该预测过程如 图所示， 当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。</p><p>&lt;br&gt;<br>&lt;div align=&quot;center&quot;&gt;<br>&lt;style&gt;.qviqmlwwdqiw{}&lt;/style&gt;<img data-src="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/02.png" class="qviqmlwwdqiw"><br>&lt;/div&gt;<br>&lt;br&gt;</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span><br><span class="line"><span class="params">                    device, save_attention_weights=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在预测时将net设置为评估模式</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [</span><br><span class="line">        src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor(</span><br><span class="line">        [tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重（稍后讨论）</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure><p></p><p>BLUE 的计算代码如下<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p></p><p>最后，利用训练好的循环神经网络 “编码器－解码器” 模型， 将几个英语句子翻译成法语，并计算 BLEU 的最终结果。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><p><code>go . =&gt; va !, bleu 1.000 i lost . =&gt; j'ai perdu ., bleu 1.000 he's calm . =&gt; il est riche ., bleu 0.658 i'm home . =&gt; je suis en retard ?, bleu 0.447</code></p><h3 id="seq2seq-小结"><a class="anchor" href="#seq2seq-小结">#</a> Seq2Seq 小结</h3><ul><li><p>根据 “编码器 - 解码器” 架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。</p></li><li><p>在实现编码器和解码器时，我们可以使用多层循环神经网络。</p></li><li><p>我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。</p></li><li><p>在 “编码器－解码器” 训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。</p></li><li><p>BLEU 是一种常用的评估方法，它通过测量预测序列和标签序列之间的 n 元语法的匹配度来评估预测。</p></li></ul><h3 id="transformer"><a class="anchor" href="#transformer">#</a> Transformer</h3><p>Transformer 架构与论文精讲已经在前面的文章中阐述过了。</p><p>基于循环或卷积神经网络的序列到序列建模方法是现存机器翻译任务中的经典方法。然而，它们在建模文本长程依赖方面都存在一定的局限性。</p><ul><li>对于卷积神经网络来说，受限的上下文窗口在建模长文本方面天然地存在不足。如果要对长距离依赖进行描述，需要多层卷积操作，而且不同层之间信息传递也可能有损失，这些都限制了模型的能力。</li><li>而对于循环神经网络来说，上下文的语义依赖是通过维护循环单元中的隐状态实现的。在编码过程中，每一个时间步的输入建模都涉及到对隐藏状态的修改。随着序列长度的增加，编码在隐藏状态中的序列早期的上下文信息被逐渐遗忘。尽管注意力机制的引入在一定程度上缓解了这个问题，但循环网络在编码效率方面仍存在很大的不足之处。由于编码端和解码端的每一个时间步的隐藏状态都依赖于前一时间步的计算结果，这就造成了在训练和推断阶段的低效。</li><li>为了更好地描述文字序列，谷歌的研究人员在 2017 年提出了一种新的模型 Transformer。</li></ul><p>Transformer 在原论文中第一次提出就是将其应用到机器翻译领域，它的出现使得机器翻译的性能和效率迈向了一个新的阶段。它摒弃了循环结构，并完全通过注意力机制完成对源语言序列和目标语言序列全局依赖的建模。在抽取每个单词的上下文特征时，Transformer 通过自注意力机制（self-attention）衡量上下文中每一个单词对当前单词的重要程度。在这个过程当中没有任何的循环单元参与计算。这种高度可并行化的编码过程使得模型的运行变得十分高效。</p><p>Transformer 的主要组件包括编码器 (Encoder)、解码器 (Decoder) 和注意力层。其核心是利用多头自注意力机制（Multi-Head Self-Attention），使每个位置的表示不仅依赖于当前位置，还能够直接获取其他位置的表示。自从提出以来，Transformer 模型在机器翻译、文本生成等自然语言处理任务中均取得了突破性进展，成为 NLP 领域新的主流模型。</p><p>从宏观角度来看，Transformer 的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为 sublayer）。第⼀个子层是多头自注意力（multi-head self-attention）汇聚；第二个子层是基于位置的前馈网络（positionwise feed-forward network）。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 位置编码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab, tgt_vocab, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.src_embedding = nn.Embedding(<span class="built_in">len</span>(src_vocab), d_model)</span><br><span class="line">        <span class="variable language_">self</span>.tgt_embedding = nn.Embedding(<span class="built_in">len</span>(tgt_vocab), d_model)</span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = PositionalEncoding(d_model, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.fc_out = nn.Linear(d_model, <span class="built_in">len</span>(tgt_vocab))</span><br><span class="line">        <span class="variable language_">self</span>.src_vocab = src_vocab</span><br><span class="line">        <span class="variable language_">self</span>.tgt_vocab = tgt_vocab</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 调整src和tgt的维度</span></span><br><span class="line">        src = src.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (seq_len, batch_size)</span></span><br><span class="line">        tgt = tgt.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (seq_len, batch_size)</span></span><br><span class="line"></span><br><span class="line">        src_mask = <span class="variable language_">self</span>.transformer.generate_square_subsequent_mask(src.size(<span class="number">0</span>)).to(src.device)</span><br><span class="line">        tgt_mask = <span class="variable language_">self</span>.transformer.generate_square_subsequent_mask(tgt.size(<span class="number">0</span>)).to(tgt.device)</span><br><span class="line"></span><br><span class="line">        src_padding_mask = (src == <span class="variable language_">self</span>.src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        tgt_padding_mask = (tgt == <span class="variable language_">self</span>.tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        src_embedded = <span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.src_embedding(src) * math.sqrt(<span class="variable language_">self</span>.d_model))</span><br><span class="line">        tgt_embedded = <span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.tgt_embedding(tgt) * math.sqrt(<span class="variable language_">self</span>.d_model))</span><br><span class="line"></span><br><span class="line">        output = <span class="variable language_">self</span>.transformer(src_embedded, tgt_embedded,</span><br><span class="line">                                  src_mask, tgt_mask, <span class="literal">None</span>, src_padding_mask, tgt_padding_mask, src_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc_out(output).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p></p><h3 id="上分技巧"><a class="anchor" href="#上分技巧">#</a> 上分技巧</h3><ul><li><p>最简单的就是调参，将 epochs 调大一点，使用全部训练集，以及调整模型的参数，如 head、layers 等。如果数据量允许，增加模型的深度（更多的编码器 / 解码器层）或宽度（更大的隐藏层尺寸），这通常可以提高模型的表达能力和翻译质量，尤其是在处理复杂或专业内容时。</p></li><li><p>加入术语词典，这是在此竞赛中比较有效的方法，加入术语词典的方法策略也有很多，如：</p><ul><li>在模型生成的翻译输出中替换术语，这是最简单的方法</li><li>整合到数据预处理流程，确保它们在翻译中保持一致</li><li>在模型内部动态地调整术语的嵌入，这涉及到在模型中加入一个额外的层，该层负责查找术语词典中的术语，并为其生成专门的嵌入向量，然后将这些向量与常规的词嵌入结合使用</li></ul></li><li><p>认真做数据清洗</p></li><li><p>数据扩增：</p><ul><li>回译（back-translation）：将源语言文本先翻译成目标语言，再将目标语言文本翻译回源语言，生成的新文本作为额外的训练数据</li><li>同义词替换：随机选择句子中的词，并用其同义词替换</li><li>使用句法分析和语义解析技术重新表述句子，保持原意不变</li><li>将文本翻译成多种语言后再翻译回原语言，以获得多样化翻译</li></ul></li><li><p>采用更精细的学习率调度策略（baseline 我们使用的是固定学习率）：</p><ul><li>Noam Scheduler：结合了 warmup（预热）阶段和衰减阶段</li><li>Step Decay：最简单的一种学习率衰减策略，每隔一定数量的 epoch，学习率按固定比例衰减</li><li>Cosine Annealing：学习率随周期性变化，通常从初始值下降到接近零，然后再逐渐上升</li></ul></li><li><p>集成学习：训练多个不同初始化或架构的模型，并使用集成方法（如投票或平均）来产生最终翻译。这可以减少单一模型的过拟合风险，提高翻译的稳定性。</p></li></ul><div class="tags"><a href="/tags/NLP/" rel="tag"><i class="ic i-tag"></i> NLP</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-12-27 22:38:17" itemprop="dateModified" datetime="2024-12-27T22:38:17+08:00">2024-12-27</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="kiyoumiii 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="kiyoumiii 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="kiyoumiii 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>kiyoumiii <i class="ic i-at"><em>@</em></i>Hexo</li><li class="link"><strong>本文链接：</strong> <a href="https://kiyoumiii.github.io/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/" title="机器翻译赛-seq2seq与transformer">https://kiyoumiii.github.io/2024/08/19/机器翻译赛-seq2seq与transformer/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;27&#x2F;ErlmUOSZGs1ypY4.jpg" title="机器翻译赛-门控循环单元GRU"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 机器学习</span><h3>机器翻译赛-门控循环单元GRU</h3></a></div><div class="item right"><a href="/2024/09/17/vue3-Router%E8%B7%AF%E7%94%B1/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;12&#x2F;27&#x2F;hYJunlxKyrf5HRE.jpg" title="vue3-Router路由"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Vue</span><h3>vue3-Router路由</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#seq2seq%E6%9E%B6%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">Seq2Seq 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#seq2seq-%E5%B0%8F%E7%BB%93"><span class="toc-number">1.1.</span> <span class="toc-text">Seq2Seq 小结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer"><span class="toc-number">1.2.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E5%88%86%E6%8A%80%E5%B7%A7"><span class="toc-number">1.3.</span> <span class="toc-text">上分技巧</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2024/08/17/%E5%9F%BA%E4%BA%8E%E6%9C%AF%E8%AF%AD%E8%AF%8D%E5%85%B8%E5%B9%B2%E9%A2%84%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%8C%91%E6%88%98%E8%B5%9B/" rel="bookmark" title="基于术语词典干预的机器翻译挑战赛">基于术语词典干预的机器翻译挑战赛</a></li><li><a href="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/" rel="bookmark" title="机器翻译赛-门控循环单元GRU">机器翻译赛-门控循环单元GRU</a></li><li class="active"><a href="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/" rel="bookmark" title="机器翻译赛-seq2seq与transformer">机器翻译赛-seq2seq与transformer</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="kiyoumiii" data-src="/images/avatar.jpg"><p class="name" itemprop="name">kiyoumiii</p><div class="description" itemprop="description">kiyoumiii's blog</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">19</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">5</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">6</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL2tpeW91bWlpaQ==" title="https:&#x2F;&#x2F;github.com&#x2F;kiyoumiii"><i class="ic i-github"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/09/17/vue3-Router%E8%B7%AF%E7%94%B1/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/deep-learning/" title="分类于 深度学习">深度学习</a></div><span><a href="/2024/07/29/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/" title="大模型基础">大模型基础</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a></div><span><a href="/2024/08/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/" title="计算机网络-基础知识笔记（一）">计算机网络-基础知识笔记（一）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/2024/08/17/%E5%9F%BA%E4%BA%8E%E6%9C%AF%E8%AF%AD%E8%AF%8D%E5%85%B8%E5%B9%B2%E9%A2%84%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%8C%91%E6%88%98%E8%B5%9B/" title="基于术语词典干预的机器翻译挑战赛">基于术语词典干预的机器翻译挑战赛</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/front-end/" title="分类于 前端开发">前端开发</a></div><span><a href="/2024/09/17/flex%E5%BC%B9%E6%80%A7%E5%B8%83%E5%B1%80/" title="flex弹性布局">flex弹性布局</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/2024/08/19/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%B5%9B-seq2seq%E4%B8%8Etransformer/" title="机器翻译赛-seq2seq与transformer">机器翻译赛-seq2seq与transformer</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/front-end/" title="分类于 前端开发">前端开发</a></div><span><a href="/2024/07/27/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80-%E4%BA%92%E8%81%94%E7%BD%91%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/" title="前端基础--互联网基本原理">前端基础--互联网基本原理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/front-end/" title="分类于 前端开发">前端开发</a></div><span><a href="/2024/08/17/%E5%89%8D%E7%AB%AF%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" title="前端编程语言">前端编程语言</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a></div><span><a href="/2024/07/29/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80-HTTP%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" title="前端基础--HTTP基本概念">前端基础--HTTP基本概念</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a></div><span><a href="/2024/07/31/%E5%89%8D%E7%AB%AF%E5%9F%BA%E7%A1%80-%E6%B5%8F%E8%A7%88%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%90%E4%BD%9C%E7%9A%84/" title="前端基础--浏览器是如何运作的">前端基础--浏览器是如何运作的</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/vue/" title="分类于 Vue">Vue</a></div><span><a href="/2024/09/17/Ant-design-vue%E4%B8%ADicon%E7%9A%84%E4%BD%BF%E7%94%A8/" title="Ant-design-vue中icon的使用">Ant-design-vue中icon的使用</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">kiyoumiii @ Yume Shoka</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">87k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">1:19</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2024/08/19/机器翻译赛-seq2seq与transformer/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->